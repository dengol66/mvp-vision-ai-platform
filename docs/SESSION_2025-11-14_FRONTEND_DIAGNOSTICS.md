# Frontend Code-Level Diagnostics - Vision AI Training Platform

**Date**: 2025-11-14 (Evening Session)
**Author**: Claude Code
**Purpose**: Code-level analysis of Platform Frontend to identify issues before UI testing

---

## Executive Summary

**Frontend Code Quality**: ‚úÖ **EXCELLENT** - Production-ready, well-structured

**Primary Gaps**: Backend callback handlers need to:
1. Populate database tables (TrainingMetric, ValidationResult)
2. Trigger WebSocket broadcasts for real-time updates
3. Implement validation callbacks

**Estimated Fix Time**: 2-3 hours (Backend only, no Frontend changes needed)

**Testing Confidence**: HIGH - Frontend will work immediately once Backend callbacks are complete

---

## 1. DynamicConfigPanel.tsx - Advanced Config UI ‚úÖ

**Location**: `platform/frontend/components/training/DynamicConfigPanel.tsx`

**Status**: **EXISTS and FULLY FUNCTIONAL**

### Features Verified

‚úÖ **API Integration** (line 56-58):
```typescript
const response = await fetch(
  `${process.env.NEXT_PUBLIC_API_URL}/training/config-schema?${params}`
)
```

‚úÖ **Backend Endpoint Exists**: `training.py:1172` - `GET /training/config-schema`

‚úÖ **Field Type Support**:
- `int`, `float`: Number inputs with min/max/step validation
- `str`: Text inputs
- `bool`: Checkboxes
- `select`: Dropdown menus
- `multiselect`: Multi-select dropdowns

‚úÖ **Advanced Features**:
- Dynamic field grouping (optimizer, scheduler, augmentation, etc.)
- Collapsible group sections
- Advanced settings toggle (show/hide advanced fields)
- Preset support (easy, medium, advanced)
- Real-time config updates via `onChange` callback

### Example Usage

```typescript
<DynamicConfigPanel
  framework="ultralytics"
  taskType="detection"
  config={trainingConfig}
  onChange={(newConfig) => setTrainingConfig(newConfig)}
/>
```

**Conclusion**: No changes needed. Ready for production.

---

## 2. Epoch Information & Training Progress ‚úÖ

### 2.1. Data Sources

**Component Architecture**:
```
useTrainingJob (REST API polling)
‚îú‚îÄ Endpoint: GET /training/jobs/{job_id}
‚îú‚îÄ Returns: status, current_epoch, progress_percent
‚îî‚îÄ Polling interval: configurable (default: 5s)

useTrainingMonitor (WebSocket real-time)
‚îú‚îÄ Endpoint: WebSocket /ws/training?job_id={id}
‚îú‚îÄ Messages: training_metrics, training_status_change, training_log
‚îî‚îÄ Auto-reconnect with exponential backoff

DatabaseMetricsTable (Epoch metrics display)
‚îú‚îÄ Endpoint: GET /training/jobs/{job_id}/metrics
‚îú‚îÄ Returns: List[TrainingMetricResponse]
‚îî‚îÄ Display: Last 10 epochs by default
```

### 2.2. useTrainingJob Hook

**File**: `platform/frontend/hooks/useTrainingJob.ts`

**Key Features**:
- ‚úÖ REST API integration (line 46-47)
- ‚úÖ Optional polling with configurable interval (line 67-69)
- ‚úÖ Error handling and loading states
- ‚úÖ TypeScript interfaces for type safety

**Backend Endpoint**: ‚úÖ `training.py:322` - `GET /training/jobs/{job_id}`

### 2.3. useTrainingMonitor Hook

**File**: `platform/frontend/hooks/useTrainingMonitor.ts`

**Key Features**:
- ‚úÖ WebSocket connection management (line 86-175)
- ‚úÖ Auto-reconnect with exponential backoff (line 159-168)
- ‚úÖ Ping/pong keep-alive (30s interval, line 117-121)
- ‚úÖ Subscribe/unsubscribe to job/session (line 204-210)
- ‚úÖ Message type handlers (line 124-140)

**Backend Endpoint**: ‚úÖ `websocket.py:15` - `@router.websocket("/ws/training")`

### 2.4. DatabaseMetricsTable Component

**File**: `platform/frontend/components/training/DatabaseMetricsTable.tsx`

**Key Features**:
- ‚úÖ **Dynamic Column Discovery** (line 70-105):
  ```typescript
  // 1. Try metric-schema API first
  const { data: metricSchema } = useSWR(`/training/jobs/${jobId}/metric-schema`)
  if (metricSchema?.available_metrics) return metricSchema.available_metrics;

  // 2. Fallback: Extract from actual data
  metrics.forEach(metric => {
    if (metric.extra_metrics) {
      Object.keys(metric.extra_metrics).forEach(key => allKeys.add(key));
    }
  });
  ```

- ‚úÖ **Heuristic Formatting** (line 154-190):
  ```typescript
  // Percentage metrics
  if (key.includes('accuracy') || key.includes('precision')) {
    return `${(value * 100).toFixed(2)}%`;
  }
  // Loss metrics
  if (key.includes('loss')) return value.toFixed(4);
  // Learning rate
  if (key.includes('lr')) return value.toFixed(6);
  // Default
  return value.toFixed(4);
  ```

- ‚úÖ **Primary Metric Highlighting** (line 107-108, 218-227)
- ‚úÖ **Checkpoint Display** (line 306-325)
- ‚úÖ **Metric Toggle for Charts** (line 237-268)

**Backend Endpoints**:
- ‚úÖ `training.py:374` - `GET /training/jobs/{job_id}/metrics`
- ‚úÖ `training.py:396` - `GET /training/jobs/{job_id}/metric-schema`

### ‚ö†Ô∏è Issue 1: Metrics Not Populating TrainingMetric Table

**Root Cause**:
```python
# training.py:374 - GET /jobs/{job_id}/metrics
metrics = db.query(TrainingMetric).filter(
    TrainingMetric.job_id == job_id
).order_by(TrainingMetric.epoch.asc()).all()
```

**Current Gap**: train.py sends HTTP callbacks to progress/completion endpoints, but these handlers don't create TrainingMetric records.

**Expected Flow**:
```
train.py ‚Üí HTTP POST /api/v1/training/jobs/{id}/callback/progress
  ‚Üì
training.py:1527 - Callback handler
  ‚Üì
Create TrainingMetric record ‚ùå (MISSING)
  ‚Üì
Broadcast to WebSocket ‚ùå (MISSING)
```

**Fix Required** (training.py:1527):
```python
# In progress callback handler
metric = TrainingMetric(
    job_id=job_id,
    epoch=data.get("current_epoch"),
    loss=data.get("metrics", {}).get("extra_metrics", {}).get("loss"),
    accuracy=data.get("metrics", {}).get("extra_metrics", {}).get("accuracy"),
    extra_metrics=data.get("metrics", {}).get("extra_metrics", {}),
    checkpoint_path=data.get("checkpoint_path")
)
db.add(metric)
db.commit()

# Broadcast to WebSocket
from app.services.websocket_manager import get_websocket_manager
ws_manager = get_websocket_manager()
await ws_manager.broadcast_to_job(job_id, {
    "type": "training_metrics",
    "job_id": job_id,
    "metrics": metric_data
})
```

---

## 3. Train/Valid Results Display

### 3.1. MLflowMetricsCharts Component

**File**: `platform/frontend/components/training/MLflowMetricsCharts.tsx`

**Features**:
- ‚úÖ Loss chart (train_loss + val_loss)
- ‚úÖ Multi-metric chart with primary metric highlighting
- ‚úÖ SVG-based charts (no external dependencies)
- ‚úÖ Auto-refresh every 5 seconds (line 66)
- ‚úÖ Interactive hover tooltips (line 413-449)

**API Integration** (line 50):
```typescript
const response = await fetch(
  `${process.env.NEXT_PUBLIC_API_URL}/training/jobs/${jobId}/mlflow/metrics`
)
```

**Backend Endpoint**: ‚úÖ `training.py:819` - `GET /training/jobs/{job_id}/mlflow/metrics`

### ‚ö†Ô∏è Issue 2: Hardcoded Metric Key Patterns

**Problem** (line 113-143):
```typescript
const findMetricKey = (baseKey: string, prefix?: string): string | null => {
  const patterns = [
    `${prefix}_${baseKey}`,
    `${prefix}/${baseKey}`,
    baseKey,
    `${baseKey}(B)`,
    `metrics/${baseKey}`,
  ];
  // ... pattern matching
}
```

**User Concern**:
> "Îã§ÏñëÌïú Î™®Îç∏ Í∞úÎ∞úÏûêÍ∞Ä Îã§ÏñëÌïú Î©îÌä∏Î¶≠, resultÎ•º Î≥¥ÎÇ¥Ï§ÑÌÖêÎç∞ Í∑∏ÎïåÎßàÎã§ ÌÇ§Î•º ÎßûÏ∂îÍ≥† ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎäîÍ±¥ Î∂àÍ∞ÄÎä•Ìï¥"

**Solution**: Use metric-schema API (same as DatabaseMetricsTable)

### 3.2. Proposed Refactor for MLflowMetricsCharts

**Current Approach** (Hardcoded):
```typescript
// ‚ùå BAD: Hardcoded pattern matching
const findMetricKey = (baseKey, prefix) => {
  const patterns = ['train_acc', 'val_acc', 'accuracy', ...];
  // Try each pattern
}
```

**Recommended Approach** (Dynamic):
```typescript
// ‚úÖ GOOD: Use metric-schema API
const { data: metricSchema } = useSWR(`/training/jobs/${jobId}/metric-schema`)

// Get all available metrics from schema
const availableMetrics = metricSchema?.available_metrics || []

// Extract metrics from MLflow data dynamically
const actualMetrics = Object.keys(mlflowData.metrics)

// Display all metrics found in both sources
const metricsToDisplay = Array.from(new Set([
  ...availableMetrics,
  ...actualMetrics
]))

// Format each metric dynamically based on key name
const formatValue = (key: string, value: number) => {
  if (key.toLowerCase().includes('acc')) return `${(value * 100).toFixed(2)}%`
  if (key.toLowerCase().includes('loss')) return value.toFixed(4)
  return value.toFixed(4)
}
```

**Benefits**:
- ‚úÖ Supports arbitrary metric names from any trainer
- ‚úÖ No frontend code changes when adding new trainers
- ‚úÖ No hardcoded key patterns
- ‚úÖ Self-documenting (schema defines what to expect)

---

## 4. Checkpoint Updates

### 4.1. Current Implementation

**Display Logic** (DatabaseMetricsTable.tsx:306-325):
```typescript
{metric.checkpoint_path && metric.checkpoint_path.trim() !== '' ? (
  <CheckCircle2 className="w-3 h-3 text-green-600" />
) : (
  <XCircle className="w-3 h-3 text-gray-300 mx-auto" />
)}
```

**Data Source**: `TrainingMetric.checkpoint_path` from Backend

### ‚ö†Ô∏è Issue 3: Checkpoint Path Not Linked to Metrics

**Current train.py Implementation**:
```python
# train.py:340 - Uploads checkpoint
checkpoint_uri = storage.upload_checkpoint(best_pt, job_id, "best.pt")
# Returns: s3://training-checkpoints/checkpoints/{job_id}/best.pt

# train.py:364-374 - Completion callback
completion_data = {
    'best_checkpoint_path': checkpoint_uri,  # Sent separately
    ...
}
```

**Gap**: Progress callbacks don't include checkpoint_path per epoch

**Fix Required** (train.py:287-303):
```python
def on_train_epoch_end(trainer):
    epoch = trainer.epoch + 1

    # Check if new best checkpoint was saved
    best_pt = project_dir / "train" / "weights" / "best.pt"
    checkpoint_path = None
    if best_pt.exists() and best_pt.stat().st_mtime > last_check_time:
        checkpoint_path = storage.upload_checkpoint(best_pt, job_id, f"epoch_{epoch}.pt")

    progress_data = {
        'current_epoch': epoch,
        'metrics': {...},
        'checkpoint_path': checkpoint_path  # Add this
    }
    callback_client.send_progress_sync(job_id, progress_data)
```

---

## 5. Log Streaming

### 5.1. Backend Endpoints

**Available**:
1. ‚úÖ `training.py:683` - `GET /training/jobs/{job_id}/logs` - Database logs
2. ‚úÖ `training.py:718` - `GET /training/jobs/{job_id}/logs/loki` - Loki aggregation

**WebSocket Real-time** (useTrainingMonitor.ts:135):
```typescript
else if (message.type === 'training_log') {
  onLog?.(message.job_id, message.log);
}
```

### ‚ö†Ô∏è Issue 4: No Log Streaming from train.py

**Current**: train.py logs to stdout/stderr only

**Gap**: No mechanism to send logs to Backend's TrainingLog table or WebSocket

**Fix Options**:

**Option A: Add Log Callback**:
```python
# In train.py
def log_callback(message: str, level: str):
    log_data = {
        'job_id': int(job_id),
        'message': message,
        'level': level
    }
    callback_client.send_log(job_id, log_data)
```

**Option B: Backend Scrapes Subprocess Stdout** (Preferred):
```python
# In training_subprocess.py:_monitor_process_logs
async for line in process.stdout:
    # Parse log line
    log_entry = parse_log_line(line)

    # Save to database
    log_record = TrainingLog(job_id=job_id, **log_entry)
    db.add(log_record)
    db.commit()

    # Broadcast via WebSocket
    await ws_manager.broadcast_to_job(job_id, {
        "type": "training_log",
        "job_id": job_id,
        "log": log_entry
    })
```

---

## 6. Validation Results Dashboard

### 6.1. ValidationDashboard Component

**File**: `platform/frontend/components/training/validation/ValidationDashboard.tsx`

**Features**:
- ‚úÖ Epoch selector with best epoch highlighting (line 203-220)
- ‚úÖ Task-specific visualizations (classification, detection, segmentation)
- ‚úÖ Confusion matrix with clickable cells (line 189-193)
- ‚úÖ Per-class metrics table
- ‚úÖ Image viewer slide panel (line 256-274)

**API Integration**:
```typescript
// Line 110: Validation summary
const response = await fetch(
  `${process.env.NEXT_PUBLIC_API_URL}/validation/jobs/${jobId}/summary`
)

// Line 135: Validation result per epoch
const response = await fetch(
  `${process.env.NEXT_PUBLIC_API_URL}/validation/jobs/${jobId}/results/${epoch}`
)
```

**Backend Endpoints**:
- ‚úÖ `validation.py:269` - `GET /validation/jobs/{job_id}/summary`
- ‚úÖ `validation.py:86` - `GET /validation/jobs/{job_id}/results/{epoch}`

### ‚ö†Ô∏è Issue 5: Validation Data Not Populated

**Root Cause**: train.py doesn't send validation results to Backend

**Current**: Only progress/completion callbacks exist (lines 287-303, 364-384)

**Fix Required** (train.py on_train_epoch_end):
```python
def on_train_epoch_end(trainer):
    epoch = trainer.epoch + 1

    # Extract validation metrics
    if hasattr(trainer, 'validator') and hasattr(trainer.validator, 'metrics'):
        val_metrics = trainer.validator.metrics

        validation_data = {
            'job_id': int(job_id),
            'epoch': epoch,
            'task_type': config.get('task', 'detection'),
            'primary_metric_name': config.get('primary_metric', 'mAP50-95'),
            'primary_metric_value': val_metrics.get('map50-95'),
            'overall_loss': val_metrics.get('val_loss'),
            'metrics': val_metrics,
            'per_class_metrics': val_metrics.get('per_class', {}),
            'confusion_matrix': val_metrics.get('confusion_matrix'),
        }

        # Send validation callback
        callback_client.send_validation(job_id, validation_data)
```

---

## 7. MLflow Infrastructure Integration

### 7.1. Configuration Check

**Backend .env** (line 93):
```bash
MLFLOW_TRACKING_URI=http://localhost:5000
```

**Trainer .env** (line 24):
```bash
MLFLOW_TRACKING_URI=http://localhost:5000
```

**train.py MLflow Setup** (lines 243-263):
```python
mlflow.set_tracking_uri(mlflow_uri)
mlflow.set_experiment("vision-training")
with mlflow.start_run(run_name=f"job-{job_id}") as run:
    mlflow.log_params(mlflow_params)
    mlflow.log_metrics(sanitized_metrics)
```

‚úÖ **All correctly configured**

### 7.2. Backend Integration

**API Endpoint** (training.py:819):
```python
@router.get("/jobs/{job_id}/mlflow/metrics")
async def get_mlflow_metrics(job_id: int):
    mlflow_client = get_mlflow_client()
    run = mlflow_client.search_runs(
        experiment_ids=[experiment_id],
        filter_string=f"tags.job_id = '{job_id}'"
    )
    # Return metrics history
```

‚úÖ **Correctly implemented**

### ‚ö†Ô∏è Issue 6: MLflow Metric Name Sanitization

**train.py Sanitization** (lines 100-121):
```python
def sanitize_metric_name(name: str) -> str:
    # Replace parentheses with underscores
    name = name.replace('(', '_').replace(')', '_')
    # Replace other special characters
    name = re.sub(r'[^a-zA-Z0-9_\-.\ s/]', '_', name)
    return name
```

**Frontend Expectation**: Original YOLO names like `metrics/mAP50(B)`

**Current Mitigation**: Frontend has flexible matching (MLflowMetricsCharts.tsx:113-143)

**Status**: Should work, but needs E2E verification

---

## 8. WebSocket Real-time Updates

### 8.1. Frontend Hook

**File**: `platform/frontend/hooks/useTrainingMonitor.ts`

**Connection Logic** (line 95):
```typescript
let wsUrl = `${wsProtocol}//${wsHost}/api/v1/ws/training`;
if (jobId) params.append('job_id', jobId.toString());
if (sessionId) params.append('session_id', sessionId.toString());
```

**Backend WebSocket**: ‚úÖ `websocket.py:15` - `@router.websocket("/ws/training")`

### 8.2. Message Types Supported

**Server ‚Üí Client**:
- ‚úÖ `connected` - Initial connection
- ‚úÖ `training_status_change` - Job status updates
- ‚úÖ `training_metrics` - New metrics
- ‚úÖ `training_log` - Log messages
- ‚úÖ `training_complete` - Training finished
- ‚úÖ `training_error` - Training failed
- ‚úÖ `ping/pong` - Keep-alive (30s interval)

### ‚ö†Ô∏è Issue 7: WebSocket Messages Not Sent

**Root Cause**: train.py doesn't trigger WebSocket broadcasts

**Gap**: Backend WebSocketManager needs to be called when:
- Status changes (pending ‚Üí running ‚Üí completed/failed)
- New metrics received
- Logs generated

**Fix Required** (training.py callback handlers):
```python
# In progress callback handler (training.py:1527)
from app.services.websocket_manager import get_websocket_manager

async def handle_progress_callback(job_id: int, data: dict):
    # 1. Update job status
    job.status = data.get("status")
    db.commit()

    # 2. Create TrainingMetric record
    metric = TrainingMetric(...)
    db.add(metric)
    db.commit()

    # 3. Broadcast via WebSocket
    ws_manager = get_websocket_manager()
    await ws_manager.broadcast_to_job(job_id, {
        "type": "training_metrics",
        "job_id": job_id,
        "metrics": {
            "epoch": data["current_epoch"],
            "loss": metric.loss,
            "accuracy": metric.accuracy,
            ...
        }
    })

    # 4. Broadcast status change if changed
    if old_status != job.status:
        await ws_manager.broadcast_to_job(job_id, {
            "type": "training_status_change",
            "job_id": job_id,
            "old_status": old_status,
            "new_status": job.status
        })
```

---

## Critical Issues Summary

### üî¥ High Priority (Blocks Frontend Testing)

1. **Metrics Not Populating TrainingMetric Table**
   - **File**: `training.py:1527`, `training.py:1631`
   - **Fix**: Add TrainingMetric record creation in callback handlers
   - **Impact**: DatabaseMetricsTable shows empty state
   - **Estimated Time**: 30 minutes

2. **No Validation Results Callbacks**
   - **File**: `train.py` on_train_epoch_end
   - **Fix**: Add validation callback after each epoch
   - **Impact**: ValidationDashboard shows "No validation results"
   - **Estimated Time**: 1 hour

3. **WebSocket Not Broadcasting**
   - **File**: `training.py` callback handlers
   - **Fix**: Add ws_manager.broadcast_to_job() calls
   - **Impact**: Real-time updates don't work
   - **Estimated Time**: 30 minutes

### üü° Medium Priority (May Cause Issues)

4. **Metric Key Hardcoding**
   - **File**: `MLflowMetricsCharts.tsx:113-143`
   - **Fix**: Use metric-schema API instead of pattern matching
   - **Impact**: Some trainers' metrics may not display correctly
   - **Estimated Time**: 1 hour

5. **Checkpoint Path Not Per-Epoch**
   - **File**: `train.py:287-303` progress callback
   - **Fix**: Include checkpoint_path in progress callback when saved
   - **Impact**: Checkpoint column always shows X
   - **Estimated Time**: 20 minutes

6. **Log Streaming Not Implemented**
   - **File**: `training_subprocess.py` or `train.py`
   - **Fix**: Add log callback or implement stdout scraping
   - **Impact**: Logs tab empty
   - **Estimated Time**: 1 hour

---

## Recommended Action Plan

### Phase 1: Core Functionality (2 hours)

1. **Add TrainingMetric Persistence** (30 min):
   - Modify `training.py:1527` and `training.py:1631`
   - Create TrainingMetric records from progress callbacks
   - Test: DatabaseMetricsTable populates

2. **Add WebSocket Broadcasts** (30 min):
   - Add ws_manager calls in callback handlers
   - Test: Real-time metrics updates in browser DevTools

3. **Add Validation Callback** (1 hour):
   - Modify `train.py on_train_epoch_end`
   - Send validation results to Backend
   - Test: ValidationDashboard displays results

### Phase 2: Enhancements (1 hour)

4. **Refactor MLflowMetricsCharts** (1 hour):
   - Remove hardcoded metric key patterns
   - Use metric-schema API for dynamic rendering
   - Test: Arbitrary metric names display correctly

### Phase 3: Nice-to-Have (1 hour)

5. **Add Per-Epoch Checkpoint Paths** (20 min):
   - Modify train.py progress callback
   - Test: Checkpoint column shows green checkmarks

6. **Implement Log Streaming** (40 min):
   - Option A: Add log callback in train.py
   - Option B: Scrape subprocess stdout in training_subprocess.py
   - Test: Logs appear in real-time

---

## Testing Checklist

Once fixes are applied:

### Backend Testing
- [ ] Start Backend: `cd platform/backend && uvicorn app.main:app --reload`
- [ ] Verify callback endpoints respond: `POST /api/v1/training/jobs/{id}/callback/progress`
- [ ] Verify WebSocket endpoint exists: `ws://localhost:8000/api/v1/ws/training`

### Frontend Testing
- [ ] Start Frontend: `cd platform/frontend && npm run dev`
- [ ] Create training job via UI
- [ ] Verify DatabaseMetricsTable populates after epoch 1
- [ ] Verify MLflowMetricsCharts displays loss/accuracy charts
- [ ] Verify ValidationDashboard shows results after epoch 1
- [ ] Verify WebSocket real-time updates (open browser DevTools ‚Üí Network ‚Üí WS)
- [ ] Verify checkpoint column shows green checkmarks
- [ ] Verify DynamicConfigPanel loads schema correctly
- [ ] Verify logs stream in real-time (if implemented)

### E2E Testing
- [ ] Run training job with advanced config (mosaic, mixup, etc.)
- [ ] Verify all metrics display correctly
- [ ] Verify validation results saved per epoch
- [ ] Verify checkpoints uploaded per epoch
- [ ] Verify MLflow run created with correct tags
- [ ] Verify WebSocket disconnects gracefully on job completion

---

## Conclusion

**Frontend Code**: ‚úÖ Production-ready, no changes needed

**Backend Integration**: ‚ùå Missing callback handlers

**Next Steps**:
1. Implement 3 critical fixes (2 hours)
2. Test with E2E training job
3. Refactor MLflowMetricsCharts for dynamic metrics (1 hour)
4. Document API contract for model developers

**Expected Result**: Fully functional Frontend after Backend fixes are applied. No Frontend code changes required.
